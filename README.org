* TrAXer

Traces Real Axion X-ray Emission Rays ? Or something like that. Who
knows, naming things is always fun huh.

This is a fork of [[https://github.com/Vindaar/rayTracingInOneWeekend][my Raytracing In One Weekend]] implementation (see [[https://raytracing.github.io/][Ray
Tracing in One Weekend]]), which majorly extends it.

It adds some features of the second book, others from the amazing
https://pbr-book.org/, but most importantly makes it a useful tool for
my work:

It is now a raytracer for X-rays (in the context of Axion helioscopes
like CAST or (Baby)IAXO).

To make it actually useful, it has an interactive raytracing mode
using SDL2, so that you can investigate the scene geometry in real
time. The standard approach to sample rays from the camera is extended
by a secondary ray sampling, which samples directly from light sources
towards ~LightTargets~. In addition with ~ImageSensors~ we can
visualize the accumulation of rays on a sensor in the scene (and save
them to binary buffers with F5).

** Show me!





** TODO Things to do

- [ ] Use colormap for the ~ttLights~ view
  -> Will this even remain? We could also implement it by just having
  the key bring the camera directly to the image sensor and/or reading
  from the ~sensorBuf~ instead of having a separate sampling proc
- [X] Refactor ~ttLights~ code to also work for multithreaded
  -> Maybe just rely on the actual ~Sensor~ buffer? And then when in
  ~ttLights~ mode we could _only_ sample rays based on the sources and
  simply display the image sensor buffer? That way we wouldn't get
  into the trouble of having to read and write from the temp buffer in
  places we don't know where they might end up on.
  - [X] the underlying buffer is now protected by a ~Lock~, both for
    read and write access
  - At this point it should be pretty much really GC safe. Each thread
    has its own RNG, ~Camera~ and ~HittablesList~. Relevant
    information between them is synced after each tracing run is done.
    - [X] ORC is still not happy though.
      -> I marked all ref types as ~acyclic~ (hittables and the
      textures). This seems to have fixed it. They are indeed
      acyclic. We have no intention of constructing ref cycles after all.
- [X] Disable sensitivity of ~ImageSensor~ to rays emitted by the
  ~Camera~ directly! (I guess they kind of act like noise haha)
  -> I gave the ~Ray~ type a ~RayType~ field that indicates if the
  initial ray was created from the ~Camera~ or directly from a light
  source.
  *NOTE*: This currently implies that if a ray is shot from the
  camera, hits a light source and then hits an image sensor, no counts
  are recorded on the image sensor! *Only* the rays we sample directly
  from light sources count.
  *BUT*: Currently our light sources act as perfect sinks anyway! They
  do not scatter light, so this scenario does not exist anyway.
- [ ] A BVH node of the telescope is currently broken! It causes all
  sorts of weirdness. The shape is correct, but the lighting is very wrong!  

- [X] make the multithreaded code safer. It's still a bit crazy.
  -> taken care of by using acyclic & a lock on the sensor
- [ ] Implement X-ray reflectivities
- [X] Check LLNL telescope setup
- [ ]


- [X] Implement ~Target~ material that is used when sampling rays for
  ~DiffuseLight~.
- [X] For parallel light use ~Laser~ instead!
- [X] Implement ~SolarEmission~ material to sample correctly based on
  flux per radius CDF

- [ ] *COULD WE* add an option to "draw" a sampled ray? I think it
  should be pretty easy!
  1. have a button, say F8, when pressed it samples a ray
  2. trace the entire ray and *mark each hit, store each intermittent
     ray*
  3. construct cylinders for each ray and overlay them on the sampled
     ray
     -> How?
     1. Use ray origin as target position for the cylinder origin
     2. Use ray direction as guide for required rotation
     3. Use (hit position - start) as length, making it end at the hit
  4. add these cylinders to the ~world~ ~HittablesList~
  5. update the ~world~ of each threads data.
  6. upon new sampling the ray should appear.
  Potential problem: The added ray interferes with the image we see on
  the ~ImageSensor~! -> Add only to the ~world~ that is used for the
  ~Camera~!

- [ ] Add ~ImageSensorRGB~ which stores the colors in each pixel. That
  way can give each layer a color, e.g. using ggplot2 colors and then
  can tell where rays from each layer end up on the sensor!  

- [ ] *ADD SANITY CHECK* that checks that in our current model indeed
  the bottom part of layer i+1 is perfectly flush with the top part of
  layer i!

*NOTE*:
The LLNL simulation is slightly wrong in the sense that the mirrors
actually become _smaller_ towards the end with smaller radius, because
we have real cone cutouts. In reality it starts from flat glass with
225 mm length and a fixed width, which is just curved to a cone
shape. That means the required 'angle' Ï†_max actually increases along
the axis. We don't model this, but the effect of this should be _very_
marginal.
See fig. 1.4 of the DTU thesis about the LLNL telescope.

